지난시간관련
full connected layer
칸이 나눠질 때 칸의 전체에대한 가중치와 bias구하기.
공간적인 정보 추출이 안되기에 해당 정보를 추출하기 위해 cnn 사용.
즉, cnn에서는 그룹핑하여 추출.
이때 합성곱을 사용.
필터, 풀링, 스트라이드, 패딩 등등 사용.
필터의 값은 처음에 랜덤으로 결정되었다 learnable rate로 인해 학습으로 최적화하며 찾아진다.
풀링은 정보를 요약하는것. 그리고 이동성 불변을 위해 사용.
stride = 합성곱을 더하는 ...
padding = ...

cnn은 시간적인 인과관계가 전혀 고려되지 않음.
그래서 rnn 모델이 나옴.
rnn의 특징 = 이전의 상태를 기억함.
즉, 과거의 상태를 출력값에 반영하자는 것.

rnn의 가장 큰 문제 = gredient vanishing problem.
성능을 높이려 네트워크 레이어를 쌓다보면 미분이 중간중간 소실될 수 있음. 그렇기에 레이어마다 업데이트가 안 될 수도 있음. 즉 뒷부분만 반영될 수 있음.
성능이 아무리 높이려해도 높아지지 않음. 이를 해결하기 위해 lstm사용.

lstm은 long short term memory, 즉 셀의 장기기억을 게이트를 통해 조절.
그러나 해당 방식은 rnn에 비해 연산속도가 매우 느려짐.
그렇기에 이를 단순화한 것이 gru.

시그모이드 중요.

p.168~169 계산 해보기
풀링도 마찬가지.

전이학습 = 잘 정의된 학습된 모델 데이터를 가져다 쓰기.
미세조정 = 학습된 것을 ...

p.246~247 간단한설명
p.259 그림 6-5

p.269 그림 6-8
p.270 표 6-2

p.290 그림 6-15
뒤의 숫자가 그만큼 블록이 많다는 것을 의미.

p.313 그림 6-24
계층이 많다고 더 좋은 성능이 아니라는 것을 의미.
skip-connection 찾아볼것.
skip-connection을 통해 기울기소멸문제, gradient vanishing problem 해결. 숏컷을 통해 해결함.
학습의 용이성.
입력이 있을 때 약간의 출력의 차이점을 통해 학습이 용이하다는 것을 의미.
정리 : 네트워크가 많다고 성능이 좋지 않으며 gradient vanihsing problem이 생김. 그렇기에 skip-connection을 사용해 문제를 해결하고 성능도 더 좋아진다는 의미.

resnet에서 발전된 것이 senet
책에 없으니 찾아볼것.
특정 채널에 집중하는 목적.

정리 : resnet은 skip-connection을 통해 gradient vanishing problem을 해결, 학습을 용이하게 함.
senet은 resnet의 발전된 모양. 정보의 중요도를 계산하고 거기에 집중하는 것.
채널의 중요도를 계산해 집중하자는것.

구글넷
인셉션 : 다양한 커널을 병렬로 뽑는것.
depth wise : 채널에 곱해지는 필터들은 다 동일했으나 하나의 데이터를 채널을 나눠서 각각의 필터를 적용하는 것. 이를 통해 성능은 더 좋아지며 연산이 줄어듬. 경량화 가능.

해당 내용들은 교수님 깃허브 variants of cnn에 정리되어있음.
channel attention module : max pooling과 average pooling을 동시에 해 mlp에서 합치고 sigmoid로 진행.
spatial attention module 등이 있고, 이러한 attention module들이 합쳐진게 트랜스포머.

p.462 정규화
p.466 과적합시의 드롭아웃.

시험범위 : 딥러닝부터 gru, transformer까지. concolusion, filter, shape, 등등. 평균풀링 계산법. mlp 순정파 계산.
senet은 범위 포함 x, 그냥 상식용. 
시험 = 17일.

senet등에서 attention이 제일 중요하며, 트랜스포머는 해당 중요도를 극대화시킨것.

인코더를 통해 정보를 줄이고 줄여 축약된 정보의 순서를 가지고 이것의 특징을 추출하고 디코더로 추출이 원래방식.
트랜스포머는 순서 상관없이 진행.
예를들어 기존의 cnn 등과 같은 경우는 가방에서 물건을 하나씩 꺼내는 방식. 트랜스포머는 가방을 부어 한꺼번에 꺼내는 방식.

self attention : 자기 내에서 중요도, 유사도를 직접 매겨 따져보는 방식.
다만 이 방식으로 진행시 O(n^2)이므로 해당 방식이 트랜스포머의 단점이라고 볼 수 있음.

cross attention : 특징들간에 중요도를 따지는 방식. 서로 무엇이 더 중요한가를 따지는거. 인코더와 디코더 사이에 구하는 것이 있음.

scaledawn attention: 키를 구별하려 1렬로 세웠을 시 한명의 특징이 너무 클 경우 비교가 잘 되지 않는 경우 의미. 그렇기에 벡터로 내적으로 나눠 전체 길이로 정규화 진행.

multihead attention : 여러관점에서 보는 것을 의미. 예를 들어 코끼리를 평가할 시 동물학적, 인류학적 등 여러 관점에서 구하는 것. 즉 가중치를 여러가지를 사용해 한꺼번에 보는 것을 의미.

교수님 깃허브 resnet_transformer.py 확인.

실습 : uci repository에서 humanactivity smartphone 검색해서 첫번째거 다운로드.
train과 test 같이되있는 특징 뽑아놓은 데이터 피쳐들의 데이터가 있으며, low데이터가 있음. 우리는 low데이터 사용. 
해당 내용은 9개 셋의 128사이즈.
batchsize에다 128,9
shape가 맞으므로 바로 cnn.
1d cnn 파일을 보고 진행. 위치만 수정해서 진행.
첫째, 간단한 cnn으로 바꿔볼것. 모델 붙이는 연습.
둘째, 지난 시간 배운것처럼 lstm으로 진행.
셋째, gru
넷째, cnn + lstm
다섯째, cnn + gru

option.
1. resnet
2. transformer
3. se
등등.









