동영상 과제 할것. 감상문. 이번주 금요일까지
사이버캠퍼스 유튜브

본수업 시작
지난시간은 cnn에 대해 배움. 
cnn과 dense layer의 차이점 = denselayer는 전체에 대한 데이터값을 추출하나 cnn은 그룹핑하여 추출. 즉 공간적인 관계, 특징을 추출하기 위해 cnn을 사용하는것. 필터별로 그룹핑한다는것이 중요한 포인트. 합성곱 등등.
필터 = 처음에 랜덤으로 생성되었다가 learnable rate로 점점 많아지고 이를 위해 pooling사용. 그래서 convolusion, pooling 반복 후 flatten으로 평탄화 이후 사용.
padding = 가장자리 처리방법.

convolusion 계산 및 pooling 계산, 필터가 언제 생성되는지 등이 시험문제.

오늘 진도 시작.
순환데이터
RNN
이전시점의 정보를 반영할것인지 아닌지를 정하는 것에 대한 CNN과의 차이점이 있음.
교수님 깃허브 RNN.GIF, 해당 그림에서 hidden units이 이전시점의 정보라고 생각하면 됨.
즉, 앞의 정보를 이용해 새로운 것을 예측하는 것.
순환데이터 : 순환신경망을 학습시키는데 사용되는 데이터를 의미.
ex : 전날 50개의 빵이 팔린 경우 다음날 100개를 미리 만들지 않는것과 같은 의미.
윈도우 사이즈가 3이면 전의 데이터 3개를 가지고 다음값을 예측.
make_sample 함수 중요.
교수님 github 예제 rnn_llstm
해당 파일에서 x,y train 등등 하고 mse 구하고 그래프 그리고 rnn, lstm 모델 구별. 

TESLA파일에서 OPEN, HIGH, LOW, CLOSE로 CLOSE예측.

LSTM RNN 완료.

순환데이터를 만들면 WINDOW사이즈만큼의 과거로 예측.

수식
h(t) = f(w)(h(t-1), x(t))
h(t) = 새로운 은닉 상태
f(w) = 가중치 w를 가지는 함수
h(t-1) = 이전 은닉 상태
x(t) = 시간 t에서의 입력 벡터

처음 alcohol은 일대일
tsla csv로 한건 다대일

다대다 = sequence to sequence 모델

rnn의 단점 = 그래디언트 소실 문제가 발생할 수 있음. 즉 장기 의존성 문제. 이를 위해 lstm으로 해결
lstm에서는 게이트웨이를 3개로 나눔. 
처음은 입력게이트, 셀에 기억을 실을것인지 말것인지 현재 정보 반영할 것인가 등등.
두번째는 삭제 게이트, 기억을 삭제하는 게이트를 의미. 시그모이드가 1이면 반영, 0이면 반영 x.
세번째는 출력 게이트, 현재 시점의 입력값과 이전 시점의 은닉 상태에 시그모이드 함수를 적용한다. 시그모이드 함수의 결과 값은 다음 시점의 은닉 상태를 결정하는 일에 사용된다. 

p. 259 그림 6-5
p. 312 ResNet  pretrend 사용 목적은 잘 훈련된 유명한 네트워크, 즉 성능이 검증된 네트워크를 가져다 씀. 앞에 데이터들을 동결시키고 뒤에값을 트레이닝하는 화인 트레이닝. 이로인해 훈련이 많이 걸리지 않는다.

7.3 순환신경망
rnn이 기존 네트워크와 다른점 = 기억을 가진다는 것.

p.393 lstm 중요
기울기 소멸 문제를 해결하기 위한 구조. 은닉노드, 즉 메모리 셀 중요. 총합을 사용하여 셀값 반영으로 인해 기울시 소멸문제 해결

p.403 그림 중요

p.418 GRU구조 중요. 성능 향상버전.

P.434 양방향 RNN 중요. 해당 방법을 통해 양쪽 다 반영 가능. 














