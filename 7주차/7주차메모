7주차 수업 메모용

시험 답안 기술방법 : 얼마ㅏ 알고있는지 쓰기. ex)경사강법 - 가중치와 bias 최적화 알고리즘. 뿐만 아닌 추가 기술. 딥러닝에서 손실을 줄이기 위해 가중치와 바이어스를 점점 줄여나가며 경사, 즉 기울기를 줄여나가는 알고리즘 기술을 의미.
중간고사 범위 : 딥러닝 전까지

딥러닝 중요요소
forward propagation

backward propagation

loss function

optimizer(SGD)

activate function
성능을 높이기 위해 레이어를 쌓아야하는데 선형을 아무리 쌓아도 특징이 잘 추출이 안되기때문에 비선형성을 중간중간에 추가하는것. sigmoid함수, relu함수(gradient vanishing problem해결법중 하나), 
다음weight = 현재 weight - n * G(손실함수 미분값), n = learning late)

weight와 bias의 최적값을 찾자는 의미

본강의 시작
사이버캠퍼스 노트 참조.
9장 컨벌루션 신경망
CNN
Dense layer 경우 3장 이미지평탄화 예시에서 보면 공간적인 관계가 깨질 가능성이 있음. 이를 해결하기 위해 gourping을 진행. 
CNN에서는 하위레이어들을 그룹핑해 부분적으로만 연결되어있음. 
즉 dense는 전체를 봐서 정보를 추출하나 cnn에서는 그룹핑해서 추출하기에 정보를 추출하기 더 좋음.
CNN은 두개의 신호를 합해서 진행하는것. 즉 여러 필터를 곱해 더 많은 정보를 추출하는 것. 합성곱 << 중요, 다시 찾아볼것.
P.13
합성곱 연산을 어떻게 하는가에 대한 얘기. 
출력레이어의 두번째 칸의 값 구하기 -> 52/9 = 5.77
(1,1)을 구할 경우 바깥 값을 0으로 채운다고 생각하면 그것을 PADDING이라고 함. 즉 padding은 테두리를 어떻게 처리할까에 대한 문제
스트라이드은 무엇인가 알아볼것 -> 보폭이라고 생각하면 됨. 칸을 몇칸씩 넘어갈것인가.

p.15 컨벌루션 연산
필터값은 backpropagation에 의해 최적값으로 찾아짐. 즉 학습에 의해 결정됨. 시간이 지나며 가중치가 학습되어 최적치 업데이트됨.

p.24
커널이 많을수록 특징이 많이 출력됨. 그렇기 때문에 pulling이 필요함. ex) 시험범위가 많을 시 요약집)
첫번째 이유 : 정보의 요약, 이동성 불변
노란색에서 가장 큰값은 9, 하늘색에서는 7, 이런식으로 최대값을 찾아서 풀링한것이 최대풀링
average풀링의 경우 노란색은 4.25, 파란색 4.25 ...

p.28 이동성불변 관련 내용
즉 비슷비슷한 특징들을 풀링한다는 의미?

convolusion -> pooling -> convolusion 반복 후 평탄화.

p.38
mnist 필기체 숫자 인식

p.41
28 x 28에서 3x3으로 필터를 통해 진행하므로 25x25 가 되고 2x2 pooling 진행하므로 13x13(12.5 반올림)으로 됨.
13x13에서 3x3으로 인해 10x10이 되고 pooling으로 인해 5x5가 됨. 이런식으로 진행됨.

책5장
합성곱 p.127
p.166중요 그림 5-1 이해할것.
dense layer를 쓰면 펼쳐져서 공간적으로 무시됨.

p.169
5-4 계산 필터 곱하기 볼것.
p.171 그림 5-11
컬러 이미지 필터가 3개면 3개 곱하고 그걸 더해 특성맵. 5-11에서는 rbg관련. 곱하고 합하게 되는것.

pooling층 : 너무 많으니 요약하자는 것을 의미

그림 5-18 denselayer와 똑같은것을 의미

p.176
2d 1d 차이점 볼것. 5-23은 3d 예시

실습 진행
denselayer 이용해 ppt p.38 진행
사이버 캠퍼스에 10-14 이번주실습 공 참고.

p.185 실습 참고.

첫번째로 28x28 cnn으로
fashion dense layer cnn 해보기
3개 분류.

추가
텐서플로에서는 파이토치를 torch.tensor로 로드함
ex)
ai-class car_evaluation_pytorch.ipynb
x_train = torch.tensor(X_train, dtype=torch.float32)
파이토치의 차이점 = valid split으로 사용할 수 없으니 프레임 벨리드, 텍스트로 바꾼다.
파이토치에서는 test와 train 두번 돌림.
파이토치에서는 backward 명시. 좀더 직관적


전이학습
pre-trained model : 잘 계산된 유명한 네트워크를 가져다 쓰는것
1. 잘 훈련된 네트워크(가중치)를 가져다 쓰자.
2. 네트워크 앞부분은 일반적인 특징, 뒤로 갈수록 구체적인 특징이므로 뒤부분을 잘라서 따로 쓰자.
3. freezing : 앞의 부분은 따로 계산을 안하고 얼림. 뒤에만 학습. 이로인해 빠른 속도를 얻음.
4. fine_tuning : 뒤에만 학습함으로인해 생기는 맞지 않는 것들을 조정하는것. 미세조정. 

  transfer learning : 서로 다른 도메인에 대해 일치시키는 것을 transfer learning이라고 말함.

책 p.183 소스
p.201 그림 5-32 해당 모델들이 pre-trained model들. 잘 훈련된 예시.
p.211 model.lesson 등등.
p.228 전이학습 , 미세조종기법. 설명 보기.
p.229 그림 5-44 중요










