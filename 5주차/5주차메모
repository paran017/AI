5주차 메모
ai개론에서 중요한 부분
딥러닝의 핵심적 부분

지난시간 중요점 =  forward propagation, backward, lossfunction, optimization, activation function
forward = 연산, 연산 후 결과 예측
backward = 포워드 반대로.
loss = 손실함수 ex : 이번달 매출액 공임비, 재료비 수익 +- 계산. 회귀에서 mse, 이중분류시 카테고리칸, 바이너리칸 등. 다중분류 손실함수, 이중분류 손실함수
optimization = 가중치 업데이트 알고리즘. 가중치 최적화하기위한 알고리즘

예시 : 밸브 3개. rgb로 이루어진 밸브. 보라색을 원할 경우 255, 0, 255로 이루어짐. 만약 내가 하늘색을 만들고 싶을 시 밸브를 돌려 하늘색을 천천히 맞춤. 이렇게 색을 맞춰가는 과정이 가중치. 이 차이를 계산하는 것이 손실함수.
       1,2,3,4,5 밸브를 조정을 해가는 것이 back propagation, 맨처음처럼 연산 후 결과 예측이 forward. 최종적인 가중치를 만드는것이 딥러닝.
       저 weight들의 최적값을 맞춰서 예상값을 잘 맞추자는 것이 딥러닝.

경사강법 : 경사를 내려가며 최적값을 찾아가는것. 
ex : weight가 14이고 손실함수를 2x^2 +2라고 할때, 가중치를 맞추는것. 다음 가중치= 현재 가중치 - 학습률 x gradient(손실함수의 미분값) y' = 4x이므로 학습률를 0.1으로 가정시, 다음 가중치 = 11.6
     다음은 6.96 등등.   (chatgpt 돌려보자)
여기서 중요점 : 경사강법은 알고리즘이다. 가중치 최적값을 찾기 위한 알고리즘. 손실함수 = 실제 값 - 예측값이다. 그럼 가중치를 어떻게 결정하느냐? 가중치 = 현재 가중치 - 학습률 x gradient. 여기서 gradient는 손실함수를 미분한 값. 즉 손실이
얼마나 나는지에 따라 가중치를 조정한다는 얘기. 손실이 0일 경우 종료.

4장 선형회귀 ppt자료
6페이지 -> 기울기 = 가중치. 선형회귀는 가중치와 절편(바이어스)를 찾는것.
5장 퍼셉트론 ppt자료
깃허브 소스 3_DL_classification_breast_cancer
입력값이 30개 뉴런이 10개. 우리가 조종해야할 가중치는 총 300개. 첫번째 뉴런에대해 30개 두번째 30개 ...
러닝레이트 0.04 등등
ppt 7페이지 퍼셉트론 볼것

활성화함수를 왜 쓰는가?

6장 MLP
성능을 올리면 layer를 여러개 쓰는것. layer를 쌓을수록 복잡한 특성 추출 가능. 하지만 선형 레이어는 많아도 쓸모가 없다. 즉 선형레이어는 아무리 많아도
하나의 레이어로 대치될 수 있다. 그래서 레이어 사이에 비선형 함수를 추가해 사용. 이러한 것의 간단한 예시는 계단함수.(5장 활성화함수에서 나옴)
9페이지 확인. 해당 과정이 forward
퍼셉트론의 한계점은 단일 하나만 쓴다는것. and나 or문제는 가능하지만 xor는 불가능. 이러한 한계점을 위해 MLP(다층 퍼셉트론)사용
5장 P.29 완벽히 이해할것
첫째 바이어스를 1.5로 하고 두번째 바이어스를 -0.5, Y를 1.5로 가정시
00일때 Y1 = 1, Y2 = 0, Y = 0
10일때 Y1 = 1 Y2 = 0 Y = 0
01일때 Y1 = 1 Y2 = 0 Y = 0
11일때 Y1 = 1 Y2 = 0 Y = 0
? 왜 0임?

2번째 시간 시작
딥러닝은 결국 각 가중치를 최소화 하고자 하는게 목적. 그 기준은 손실함수. 손실함수 미분 = gradient 그래디언트 = 전체 손실에 따른 가중치를 어떻게 설정할것인지
다음 가중치 = 현재 가중치 - 학습률 * gradient
경사강법 : 손실함수를 미분한 gradient를 이용해 가중치를 찾아가는 알고리즘.

ACTIVATION FUNCTION
6장 p.17 참고
6장 p.33 참고
6장 p.45 참고
p.46 총오차 계산

3장
이미지 분류
p.45 MNIST 데이터
28X28 
