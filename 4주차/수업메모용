지난시간 머신러닝 주요개념
서포트벡터 : 예를들어 2개의 집단이 있는데 그 사이의 마진을 벌릴수록 그 경계로 더 분류가 잘 되는 모델. 기준선을 통해 데이터 분류.
decision tree : 데이터를 분류하거나 결괏값 예측
랜덤 포레스트 : decision tree의 집합. 샘플을 중복을 허락해 여러가지 샘플을 만드는것.(ex : 12344, 12357, 13579) 앙상블은 랜덤포레스트중 좋은걸 선택해 사용
knn : 레이블을 결정하는것. 자기 주변을 보고 자신의 레이블을 선택하는것.

본수업 시작
samplet 편향 : train데이터와 test데이터를 나눌때 편향이 존재할 수 있다는 것. a와 b로 나눌때 7:3비율로 있을 때 트레인 데이터는 5:2고 test데이터가 다르게 나올때
      이러한 성능이 안좋게 나오는 것을 클래스에 따라 해당 비율을 맞추는 것을 의미. 불균형한 데이터를 어떻게 해결할지 생각할때 필요한것.
혼공머신 실습예제 2-2
scaling : 해당 예제에서 스케일링 전에는 도미인지 빙어인지 예상이 안가는 경우가 있는데 스케일링을 통해 해당 문제를 없애는것을 의미. 단 테스트와 트레인 값의 분포가 다르기때문에 각각 맞춰서 해야됨.
scaling 공식 : 각 데이터 - 평균/분산
결정계수 : (타깃-예측)^2의 합 / (타깃-평균)^2의 합 3-1예제 참고. out[15] 볼것.

과소적합 과대적합 (시험 가능성)
언더핏 : 학습이 덜 된 것(과소적합)
오버핏 : 학습이 너무 많이 된 것(과대적합), 데이터에 민감해지니 쓸데없는 정보(노이즈)도 입력.
언더핏 해결방법 : 모델을 늘리거나 등등.
오버핏 해결방법 : 모델을 단순화시키거나 데이터를 단순화시키거나.
l1nom = 피쳐를 없애는것.
l2nom = 가중치를 없애는것.

3-2예제
linear regression 관련.
13라인에서 절편과 기울기가 구해짐. 앞의 값이 기울기, 뒤에값이 절편
해당 값들로 1차 방정식 그래프와 산점도를 그림. 즉 데이터들의 값을 이용해 기울기와 절편을 구해내는 것을 의미.
여기에서 기울기 = 가중치, 그리고 딥러닝 = 바이어스
강의자료 5장 퍼셉트론
퍼셉트론 = 입력 2개를 받아들여 출력값이 나오는 것. 
bias = 모델이 기본적으로 가지고 있는 성향.

예제 4-1
numpy 와 pandas
numpy는 계산이 빠르나 칼럼명이 없어 후 처리에 불편함.
numpy를 pandas로 바꿀수도 있고 반대의 경우도 가능.

로지스틱회귀
시그모이드 함수.
로지스틱회귀란 이름은 회귀나 분류 모델. 시그모이드 = 로지스틱펑션
4-1 예제의 27라인

클리너 알고리즘?
5-1예제
진위불순도


교차검증과 그리드서치(5-2예제)
하이퍼파라미터 튜닝
하이퍼파라미터 = 모델에 들어가는 여러가지 입력 변수들.
ex : 축구팀에서 3백은 피지컬이 세고 윙어는 빨라야함. 스트라이커는 결정적인 골 결정력이 있어야함. 이러한 조합으로 최적의 팀을 만드는것과 같은 예제를 하이퍼 파라미터 튜닝이라고 함.

winequality white wine
pd.readexcel 
logistic regression, randomforest, knn, deicisontree
기울기 절편값 구하
기울기 절편은 혼공책 실습예제 3-2 참조
print(lr.coef_, lr.intercept_) 이거


딥러닝 시작
만들었던 소스로 딥러닝 시작.

우선 예제는 교수님 aiclass  DL_Classification_sample.ipynb파일

1. 딥러닝 vs 머신러닝

x y -> x_train, x_test, y_train, y_test 여기는 공통사항

1)딥러닝에서는 label을 one-hol-encoding으로 바꾼다. one-hol-encoding은 012 를 001 010 100 으로 바꾸는 형식. label사이의 상관관계를 없애기 위해 이러한 방식 채용.
1번이 고양이, 2번이 개, 3번이 소일 경우 개와 고양이의 경우 소보다 더 가까우므로 bias에 영향을 끼침. 
2)딥러닝은 train데이터를 train과 validation 으로 나눔. (용어 확실치 않으니 찾아볼것)
cross validation(예를 들어 10명이 있고 1명이 테스트일때 10번의 테스트를 돌려 여러번 교차검증하는것)
3) x값도 numpy로 바꿔줘야한다. 

소스기준 
앞의 틀은 똑같으나 7번라인에서 one-hol-encoding으로 바꾸는 예시. 이로인해 y의 label을 바꾸는 것.
8번라인의 x=x.value가 numpy로 바꾸는것.

16번라인 dropout 필요 없음.
15번라인에서 input 에 피쳐의 개수. 피쳐의 개수 신경쓰기 싫으면 숫자 없애면 됨
와인의 경우 18라인의 y.shpae가 7. 신경쓰기 싫으면 숫자 없애면 됨.
softmax 찾아보자

27라인의 history = model.fit이 fit하는것. 20%떼서 알아서 만들어줘라 뭐 이런 의미. 

교수님 코드 바탕으로 wine 다시 해보기
breast_cancer 코드바탕으로 해보기. 과제

note 4장 선형회귀 이론수업

loss를 줄이며 기울기를 구해나가는게 중요쟁점.
딥러닝은
1. 순정파 forward profication
2. backward profication
3. loss function
4. activate function
5. optimization
위 5가지가 중요. 특히 lossfunction이 제일 중요

이 중에서 loss function 설명 중요. mse 도 loss function의 예시. 
데이터 입력 후 기울기를 구하는것이 순정파(forward profication), 이로인해 나온 1차그래프로 손실값을 구해 기울기 조정. 이런것들을 계속 업데이트해 손실을 줄여나가는게 loss function
mse가 하나의 예시. back profication은 이러한 weight를 조정할때 얼마나 조정할까를 정하는 것.appoach는 이러한 조정을 몇번 했는지를 카운트하는것.
즉 포워드로 손실값 구하고 이러한걸로 backward를 하고, 반복하는것. 즉 머신러닝은 1번만 진행해 손실을 조정할수 없으나 딥러닝은 계속 반복해서 손실을 줄여나가는 형식.
forward -> loss -> backward -> optimizor






